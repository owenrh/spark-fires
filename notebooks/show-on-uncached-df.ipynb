{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe201081-e32f-4a2a-be4d-fdc30da349f7",
   "metadata": {},
   "source": [
    "![title](img/this-is-fine-spark.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a53a34-4172-4fa5-afa0-7a39e00cbfdf",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Spark fires ðŸ”¥ - show() on uncached dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f15141-5017-40dc-b604-ff226252a6c7",
   "metadata": {},
   "source": [
    "In this scenario we create call `df.show()` on a dataframe which has not been cached. This causes recomputation of part of our DAG, slowing our job down."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d30bfbe-c0b9-4b04-a490-e039211708b0",
   "metadata": {},
   "source": [
    "### Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5156a-cd85-45bb-8d79-0c614a523552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder.master(\"spark://spark:7077\")\n",
    "    # .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    # .config(\"spark.eventLog.dir\", \"/data/tmp/spark-events\")\n",
    "    .appName(\"spark-fires-show-on-uncached-df\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e62198-1e64-4529-9049-a9df750e9992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = spark.range(0, 7200 * 6).repartition(12).cache()\n",
    "df2 = df.withColumn('created_at', F.current_timestamp()).repartition(df.rdd.getNumPartitions() * 2).cache()\n",
    "df2.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628fc1f-631d-48c2-9f66-d1b891a4cf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from time import sleep\n",
    "import os\n",
    "\n",
    "def process_partition(iterator):\n",
    "    for item in iterator:\n",
    "        sleep(0.01)\n",
    "        yield item\n",
    "\n",
    "mapped = df.rdd.mapPartitions(process_partition).toDF()  #.cache()\n",
    "joined = mapped.join(df2, 'id') #.cache()  # -- change 1\n",
    "joined.show(truncate=False)  # -- change 2 - just remove the show call altogther, and make sure we are not caching in `-- change 1`\n",
    "joined = joined.withColumn('one_up', F.col('id') + 1)\n",
    "joined.write.format(\"parquet\").mode('overwrite').save(\"/data/range_nums\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff37d660-a00c-476e-8517-ef3f36342316",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.clearCache() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a030cf-0283-471d-8940-0ec81f0fe700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb5f13-45c8-4704-ba21-e1a171d43654",
   "metadata": {},
   "source": [
    "### Putting the fire out  ðŸ”¥ðŸ”¥ðŸ”¥ ðŸš’ ðŸš’ ðŸš’ ðŸ§¯ðŸ§¯ðŸ§¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a614d62-904c-4bab-87c2-5faadad762d3",
   "metadata": {},
   "source": [
    "First run through I see an execution time of ~ 2 min 15 secs seconds. If you look at the Spark UI on http://localhost:4040/ and when we dig through into the jobs we see the following:\n",
    "* http://localhost:4040/jobs/job/?id=7 - our show() call at ~ 45s\n",
    "* http://localhost:4040/jobs/job/?id=9 - our save() call at ~ 1m 30s \n",
    "\n",
    "So that's fine right? Well, no. Our `show()` call is recomputing part of the DAG costing us time. Let's look at our options:\n",
    "1. Firstly, we can leave the `show()` call in there but cache the target dataframe, so that work materialised during the show call is saved and is reused, rather than being recomputed from scratch. Try uncommenting `-- change 1` for this. Boom, our runtime drops to around ~ 1m 30s and we see **> 30% drop in runtime**.\n",
    "1. Next, we can look at the show call itself. It is rarely a good idea to leave debug code in your app code. Let's remove it `-- change 2` and the dataframe caching `-- change 1`. In this case, with our noddy test scenario the runtime reduction is about the same as for our last change. However, in a real-world example our data would probably be much larger so we would take a bigger performance hit for caching it, and may not even be able to fit it into memory.\n",
    "\n",
    "In some ways you could argue this is a 'noobie' mistake but I have seen multiple examples of this in productions jobs: some of these had multiple _show_ calls in which accounted for > 60% of their runtime ðŸ˜®."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1b673-7ed0-4715-a90f-2aac52ff1b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
