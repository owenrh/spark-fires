{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c5ffd1-b04c-4b5c-8287-30bab6143ad6",
   "metadata": {},
   "source": [
    "![title](img/this-is-fine-spark.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bfad4-524e-48c2-a97f-9841ace4b215",
   "metadata": {},
   "source": [
    "## 🔥 Spark fires 🔥 - file formats are not created equal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd267b5f-7435-43c2-8262-b76bf639330c",
   "metadata": {},
   "source": [
    "... or CSV and JSON be like meh 👎\n",
    "\n",
    "In this scenario, we will look at why file-formats matter, and specifically why you want to avoid formats like CSV and JSON where possible. \n",
    "\n",
    "We'll even count rows. Not because it's exciting but because you end up doing it more than you'd imagine due to data reconciliation, etc. 😴\n",
    "\n",
    "Note, for this scenario we will have two options:\n",
    "1. Use the bike-data datasets, which are a little on the small side.\n",
    "2. Generate some **synthetic data which will take up ~ 30G on your hard drive**.\n",
    "\n",
    "*Consider yourself warned!* 😅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21954162-62be-4791-8e29-8a57fe38f07b",
   "metadata": {},
   "source": [
    "### Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5156a-cd85-45bb-8d79-0c614a523552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.12:3.5.0  pyspark-shell'\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder.master(\"spark://spark:7077\")\n",
    "    # .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    # .config(\"spark.eventLog.dir\", \"/data/tmp/spark-events\")\n",
    "    .appName(\"spark-fires-more-cores-than-partitions\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# option 1 - use small data - uncomment to use\n",
    "# input_data_path_original = '/data/bike-data/201508_trip_data.csv'\n",
    "# input_data_path_csv = '/data/bike-data/201508_trip_data_underscored.csv'\n",
    "# input_data_path_json = '/data/bike-data/201508_trip_data.json'\n",
    "\n",
    "# # note, we supply a schema to avoid schema inference OOM issues with JSON\n",
    "# schema = schema = StructType([\n",
    "#     StructField(\"TripID\", StringType(), False),\n",
    "#     StructField(\"Duration\", IntegerType(), False),\n",
    "#     StructField(\"StartDate\", TimestampType(), False),\n",
    "#     StructField(\"StartStation\", StringType(), False),\n",
    "#     StructField(\"StartTerminal\", StringType(), False),\n",
    "#     StructField(\"EndDate\", TimestampType(), False),\n",
    "#     StructField(\"EndStation\", StringType(), False),\n",
    "#     StructField(\"EndTerminal\", StringType(), False),\n",
    "#     StructField(\"BikeNum\", IntegerType(), False),\n",
    "#     StructField(\"SubscriberType\", StringType(), False),\n",
    "#     StructField(\"ZipCode\", StringType(), False)\n",
    "# ])\n",
    "\n",
    "# option 2 - use bigger data which requires a little more legwork to generate\n",
    "# (note, you'll need ~ 30G or so free for this)\n",
    "input_data_path_csv = '/data/G1_1e8_1e8_100_0.csv'\n",
    "input_data_path_json = '/data/G1_1e8_1e8_100_0.json'\n",
    "\n",
    "# note, we supply a schema to avoid schema inference OOM issues with JSON\n",
    "schema = schema = StructType([\n",
    "    StructField(\"id1\", StringType(), False),\n",
    "    StructField(\"id2\", StringType(), False),\n",
    "    StructField(\"id3\", StringType(), False),\n",
    "    StructField(\"id4\", IntegerType(), False),\n",
    "    StructField(\"id5\", IntegerType(), False),\n",
    "    StructField(\"id6\", IntegerType(), False),\n",
    "    StructField(\"v1\", IntegerType(), False),\n",
    "    StructField(\"v2\", IntegerType(), False),\n",
    "    StructField(\"v3\", DecimalType(), False)\n",
    "])\n",
    "\n",
    "input_data_path_parquet = '/data/bike-data-parquet'\n",
    "input_data_path_avro = '/data/bike-data-avro'\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768cbca-01b7-4775-b5aa-575067e1fda9",
   "metadata": {},
   "source": [
    "### Let's prep our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd23cc2-dd1a-40cd-b863-7bf10a2e09da",
   "metadata": {},
   "source": [
    "So for this we have two options:\n",
    "1. Use a pre-existing but pretty small dataset.\n",
    "1. Do a little more legwork to create a larger synthetic dataset, which will better demonstrate the differences between the formats.\n",
    "\n",
    "I will shoot for option 2, but if you are feeling lazy feel free to take the easier option. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d4ed9b-cefc-4bf3-9d74-e23ec85d8a4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Option 1 - the small dataset 👎"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d9727-871d-41db-8e8a-5586ed5f3a7f",
   "metadata": {},
   "source": [
    "We are going to borrow some test data from the excellent _Spark, The Definitive Guide_ Git repo. \n",
    "\n",
    "Make sure to go back to the cell that creates the session and switch up the input file paths for this to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5327d-3368-452d-923c-4307d7e771b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p /data/bike-data\n",
    "# !wget https://raw.githubusercontent.com/udacity/data-analyst/master/projects/bike_sharing/201508_station_data.csv -P /data/bike-data\n",
    "# !wget https://raw.githubusercontent.com/udacity/data-analyst/master/projects/bike_sharing/201508_trip_data.csv -P /data/bike-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df7a28-6aa9-499b-82e6-4fd3fe377451",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /data/bike-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3793616-a6db-49ae-90b1-238626cdd718",
   "metadata": {},
   "source": [
    "Then we are going to write the CSV data down in the following additional formats: JSON, Parquet and Avro. \n",
    "\n",
    "*Note, for the CSV and JSON formats we will have just a single input file - as this is how you will often encounter these formats in the wild, e.g. if no-one could be bothered to use a better format they probably also could not be bothered to write down multiple file-splits.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fd7fb2-8c80-4ff4-b71a-e35a0c409947",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(input_data_path_original)\n",
    "df.columns = df.columns.str.replace(' ', '')\n",
    "df.columns = df.columns.str.replace('#', 'Num')  # note, we remove the spaces from the column names as Avro will not deal with those : o\n",
    "df.to_csv(input_data_path_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67706a50-2ed1-44ea-8a75-e1ffa1b01ab3",
   "metadata": {},
   "source": [
    "#### Option 2 - the bigger dataset\n",
    "\n",
    "For this we are going to use [falsa](https://github.com/mrpowers-io/falsa), a synthetic data generation tool created by Matthew Powers. \n",
    "\n",
    "To use this you will need to follow this rough approach:\n",
    "1. Create a new venv, with your preferred tool, e.g. pyenv, etc.\n",
    "2. Install falsa as per the README.\n",
    "3. Then run the following command from your spark-fires directory:\n",
    "\n",
    "`\n",
    "falsa groupby --path-prefix=./data --size MEDIUM\n",
    "`\n",
    "\n",
    "After running this you should see that you have a fairly chunky file with about 6.4G and 100k rows of data. Result. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a12f0d-5376-433d-81fe-d4ffeb568dc8",
   "metadata": {},
   "source": [
    "#### Let's just write down the JSON, Parquet and Avro versions of the CSV\n",
    "*Note, this take about 4 mins on my machine for the bigger synthetic dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e073a-f9c0-435c-b40b-a2f5bae06ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tmp_data_path_json = '/data/tmp-json'\n",
    "\n",
    "df = spark.read.option(\"header\", True).schema(schema).csv(input_data_path_csv)\n",
    "df.repartition(1).write.format('json').mode('overwrite').save(tmp_data_path_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9b109a-f731-4acb-beb7-934694c94ca5",
   "metadata": {},
   "source": [
    "Okay, let's rename our file-split that Spark has created, to make things easier to manage. *Look away friends, just a bit of filename hacking going on* 🙈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20c2d78-a91c-46c5-a676-8eb5fd9e0c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "dir_itr = Path(tmp_data_path_json).iterdir()\n",
    "\n",
    "file_path = next(dir_itr)\n",
    "while os.path.isdir(file_path) or str(file_path).endswith('.crc'):\n",
    "    file_path = next(dir_itr)\n",
    "\n",
    "file_path\n",
    "os.rename(file_path, input_data_path_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade485c-451a-4b2a-b111-a63ea29ac08e",
   "metadata": {},
   "source": [
    "Let's sort the Parquet and Avro - note this will take > 5 mins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d96298-68ea-498f-b9b9-9d84f9511a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /data/bike-data-parquet\n",
    "# !rm -rf /data/bike-data-avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b343ab3-3e5e-48ab-ac64-e61998b708d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def get_csv():\n",
    "    return spark.read.option(\"header\", True).schema(schema).csv(input_data_path_csv)\n",
    "\n",
    "if not os.path.exists(input_data_path_parquet):\n",
    "    df = get_csv()\n",
    "    df.repartition(12).write.format('parquet').save(input_data_path_parquet)\n",
    "\n",
    "if not os.path.exists(input_data_path_avro):\n",
    "    df = get_csv()\n",
    "    df.repartition(12).write.format('avro').save(input_data_path_avro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e11933-39fb-4f24-84c8-894d6cec3d1b",
   "metadata": {},
   "source": [
    "### Data formats at rest 💾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c38ea-68c1-4b3f-9049-cc008c7030bc",
   "metadata": {},
   "source": [
    "#### ... digging into the details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc34c96b-4b06-47aa-8474-c82c5101dd10",
   "metadata": {},
   "source": [
    "Okay, before we do any processing let's have a look at what these formats look like on disk, at rest. There can't be that much difference right? 🤷"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27800e99-18f8-442c-9a89-1f81df39ec12",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /data/bike-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff95588f-9609-4683-be6b-1971ec333724",
   "metadata": {},
   "source": [
    "Note, the false synthetic data should be a CSV file named **G1_1e8_1e8_100_0.csv**, if you have gone with the default option 2 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f85c08d-8d5a-4081-aaa0-61413c57ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8123ade0-5b74-46f8-abb4-4f7523ec1658",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /data/bike-data-avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b6687c-a23e-449c-a0ab-94857ce226d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh /data/bike-data-avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73019715-2038-4622-be6c-652e1863f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lh /data/bike-data-parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ea46c6-d13f-47af-b051-85e4c95ca56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -sh /data/bike-data-parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70d373d-b345-4165-b92d-f4c357d3d396",
   "metadata": {},
   "source": [
    "#### ... say whaaat 🔥🔥🔥 🚒 🚒 🚒 🧯🧯🧯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32510a88-bb9a-4562-be8f-27145b7b2a34",
   "metadata": {},
   "source": [
    "Ahem, so the first thing we can see is that there is a reasonable difference in the space taken up by these formats:\n",
    " * CSV ~ 6.5G\n",
    " * JSON ~ 10G\n",
    " * Parquet ~ 2.1G\n",
    " * Avro ~ 3.1G\n",
    "\n",
    "So with respect to the original CSV dataset:\n",
    " * JSON is > 50% bigger\n",
    " * Parquet is ~ 70% smaller\n",
    " * Avro is > 50% smaller\n",
    "\n",
    "In this instance, Parquet is a clear winner. It is obviously worth noting this will vary massively with each dataset. For instance, with the original bike-data dataset the **CSV is a whopping x4 on-disk and JSON is x8** compared with the Parquet - oof. Not ideal for your object storage bills. 😮 💰💰💰🔥🔥🔥\n",
    "\n",
    "The binary formats, Parquet and Avro, clearly have smaller footprints. Parquet will generally have an advantage on a bigger dataset which contains more enumerations due to columnar column encoding (which is a bit beyond the scope of this notebook to go into, you can [read more here though in Alex Merced's excellent overview](https://medium.com/data-engineering-with-dremio/all-about-parquet-part-06-encoding-in-parquet-optimizing-for-storage-b857ebfcf9a9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d4ddf-e4e8-4669-9955-b0e3f6ca5165",
   "metadata": {},
   "source": [
    "### Now let's do some data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef501583-6148-4b97-b456-7112c2192649",
   "metadata": {},
   "source": [
    "#### First off let's just count the rows\n",
    "Don't forget to **first restart the kernel**, so we don't have any cached file references, metadata, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fdfaec-cd19-4543-9be9-9a468864b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = spark.read.csv(input_data_path_csv)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee35d1e6-b961-4074-9457-6796c538ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# note, we supply a schema to avoid schema inference OOM issues with JSON\n",
    "df = spark.read.schema(schema).json(input_data_path_json)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fd5573-7d40-4745-bf74-ceed8c24a0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = spark.read.parquet(input_data_path_parquet)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b0354c-8dfe-4f74-b2dd-d50189843f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = spark.read.format('avro').load(input_data_path_avro)\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae96f0-3cee-4ca4-a63f-74627fa99546",
   "metadata": {},
   "source": [
    "#### Uh, counting be like *wa wa waaa* 🔥🔥🔥 🚒 🚒 🚒 🧯🧯🧯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eda4e3b-879e-497a-9ce8-92945919a285",
   "metadata": {},
   "source": [
    "So even counting shows a considerable difference between the file formats:\n",
    " * CSV ~ 20 secs\n",
    " * JSON ~ 36 secs (although we gave it a hand by supplying a schema)\n",
    " * Parquet ~ 1.6 secs\n",
    " * Avro ~ 15 secs\n",
    "\n",
    "The clear a winner here as **Parquet is ~ 9 - 22 times faster than the other formats**. This is because Parquet stores various pieces of metadata in file footers so that the counts can be determined by reading a very small amount of the files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c34eaa-4e3e-47b4-8a98-db6522132015",
   "metadata": {},
   "source": [
    "#### Okay, let's throw in a filter with our count\n",
    "Don't forget to **first restart the kernel**, so we don't have any cached file references, metadata, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3c334-159d-46ff-bca5-9291306c8568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.summary('min', '25%', '50%', '75%', 'max').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51c3f1-e31c-44d3-a192-88507af0c943",
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_filter = F.col('v3').between(47, 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc77794-0dcf-41bd-8e4e-3feef6e0d055",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = spark.read.option('header', 'true').schema(schema).csv(input_data_path_csv)\n",
    "df.filter(bounds_filter).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0952b-291c-4998-a280-cc1b0e69a934",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = spark.read.schema(schema).json(input_data_path_json)\n",
    "df.filter(bounds_filter).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f1e4d2-022a-4972-9b72-7feb0aff72ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = spark.read.parquet(input_data_path_parquet)\n",
    "df.filter(bounds_filter).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85769c6-2511-49af-b0dc-664eeee8f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = spark.read.format('avro').load(input_data_path_avro)\n",
    "df.filter(bounds_filter).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cc429f-93e2-43f3-ae2b-01bc902004ef",
   "metadata": {},
   "source": [
    "#### Parquet wins the filter wars 🔥🔥🔥 🚒 🚒 🚒 🧯🧯🧯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9239f4-01db-4a93-8547-f55805b9ee74",
   "metadata": {},
   "source": [
    "So with a filter thrown in the counts break down as follows:\n",
    " * CSV ~ 36 secs\n",
    " * JSON ~ 29 secs (although we gave it a hand by supplying a schema)\n",
    " * Parquet ~ 2.5 secs\n",
    " * Avro ~ 17 secs\n",
    "\n",
    "The clear a winner here again as **Parquet is ~ 7 - 15 times faster than the other formats**. This is footer metadata previously mentioned and columnar storage layout which minimizes the data read. Boom, Parquet for the win. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277dfb48-4a56-4628-a47a-d88200e72d54",
   "metadata": {},
   "source": [
    "#### Right let's do some actual processing\n",
    "Don't forget to **first restart the kernel**, so we don't have any cached file references, metadata, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72add9cb-6a75-4c86-a02e-746c4c574343",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_the_data(df):\n",
    "    df.groupby('id1').avg('v3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1185d-f503-4cad-a16a-80c98f3f24b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = spark.read.option('header', 'true').schema(schema).csv(input_data_path_csv)\n",
    "agg_the_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb01f94-edd2-494f-a205-1f968b698de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = spark.read.schema(schema).json(input_data_path_json)\n",
    "agg_the_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df791db-cb6e-45f9-bc67-97be5c0f14b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = spark.read.parquet(input_data_path_parquet)\n",
    "agg_the_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972e773-0537-496f-9014-46e9b1227c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "df = spark.read.format('avro').load(input_data_path_avro)\n",
    "agg_the_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8753b-f8f4-4e37-ad1a-801740145d98",
   "metadata": {},
   "source": [
    "### ... and the winner is ... 🏆🏆🏆"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c49917-7ce6-4dd2-97b6-bf790f02631e",
   "metadata": {},
   "source": [
    "So basic agg breaks down as follows:\n",
    "* CSV ~ 43 secs\n",
    "* JSON ~ 38 secs \n",
    "* Parquet ~ 4 secs\n",
    "* Avro ~ 21 secs\n",
    "\n",
    "So again Parquet is the clear winner. **Parquet is ~ 5 to 10 times faster** on this task which is because of filter pushdown and it's columnar layout. This results in the Parquet tasks reading very much less data.\n",
    "\n",
    "Obviously, these differences will vary considerably with datasets. Our test datasets have few columns, if we were had a hundred of more columns in play the differences in performance would be monumental. Also, remember this is all local, so we are missing network and object storage latencies, which would again favour Parquet.\n",
    "\n",
    "### In summary\n",
    "So what can we say in summary? Formats like CSV and JSON are not *bad* per se. But binary, and particularly columnar formats, have far superior performance characteristics in the analytics space where we are often targeting computation at a subset of partitions, rows and columns. \n",
    "\n",
    "As a row-based format, Avro is commonly seen in stream-processing where we are reading and processing whole records at a time and where columnar formats offer no advantage.\n",
    "\n",
    "Parquet dominated all of our tests by quite some way. This is why it is the defacto standard format of modern data processing, and is utilised under the hood by the newer data lake formats like Apache Iceberg and Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a030cf-0283-471d-8940-0ec81f0fe700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
