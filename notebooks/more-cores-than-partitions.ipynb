{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c5ffd1-b04c-4b5c-8287-30bab6143ad6",
   "metadata": {},
   "source": [
    "![title](img/this-is-fine-spark.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bfad4-524e-48c2-a97f-9841ace4b215",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Spark fires ðŸ”¥ - more cores than partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd267b5f-7435-43c2-8262-b76bf639330c",
   "metadata": {},
   "source": [
    "In this scenario, we will demonstrate how not having enough in-memory partitions can lead to you not using all the available executor cores.\n",
    "\n",
    "For this experiment, we will create a small number of input file-splits to highlight the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21954162-62be-4791-8e29-8a57fe38f07b",
   "metadata": {},
   "source": [
    "### Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5156a-cd85-45bb-8d79-0c614a523552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder.master(\"spark://spark:7077\")\n",
    "    # .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    # .config(\"spark.eventLog.dir\", \"/data/tmp/spark-events\")\n",
    "    .appName(\"spark-fires-more-cores-than-partitions\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768cbca-01b7-4775-b5aa-575067e1fda9",
   "metadata": {},
   "source": [
    "### Let's prep our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d9727-871d-41db-8e8a-5586ed5f3a7f",
   "metadata": {},
   "source": [
    "We are going to borrow some test data from the excellent _Spark, The Definitive Guide_ Git repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5327d-3368-452d-923c-4307d7e771b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p /data/bike-data\n",
    "# !wget https://raw.githubusercontent.com/udacity/data-analyst/master/projects/bike_sharing/201508_station_data.csv -P /data/bike-data\n",
    "# !wget https://raw.githubusercontent.com/udacity/data-analyst/master/projects/bike_sharing/201508_trip_data.csv -P /data/bike-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df7a28-6aa9-499b-82e6-4fd3fe377451",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data/bike-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd41dfe-f1d8-4e20-b6e5-fa909dbdfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path_2s = '/data/bike-data-2-splits'\n",
    "input_data_path_12s = '/data/bike-data-12-splits'\n",
    "output_data_path = '/data/bike-data-partitioned-out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99481820-7b61-4a96-96a4-fde3e3ca8f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf /data/bike-data-2-splits\n",
    "# !rm -rf /data/bike-data-12-splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee650c-e678-4649-bdb5-17355dc04643",
   "metadata": {},
   "source": [
    "Next we will create some input data with two file splits for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b343ab3-3e5e-48ab-ac64-e61998b708d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 0.25\n",
    "\n",
    "if not os.path.exists(input_data_path_2s):\n",
    "    df = spark.read.option(\"header\", True).csv(\"/data/bike-data/201508_trip_data.csv\")\n",
    "    df = df.sample(fraction=sample_size)\n",
    "    df.repartition(2).write.format('parquet').save(input_data_path_2s)\n",
    "\n",
    "if not os.path.exists(input_data_path_12s):\n",
    "    df = spark.read.option(\"header\", True).csv(\"/data/bike-data/201508_trip_data.csv\")\n",
    "    df = df.sample(fraction=sample_size)\n",
    "    df.repartition(12).write.format('parquet').save(input_data_path_12s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73019715-2038-4622-be6c-652e1863f0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /data/bike-data-two-splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd11bd69-abc7-4220-8855-a860784f3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ls -lh /data/bike-data-12-splits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d4ddf-e4e8-4669-9955-b0e3f6ca5165",
   "metadata": {},
   "source": [
    "### Now let's do some data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c34eaa-4e3e-47b4-8a98-db6522132015",
   "metadata": {},
   "source": [
    "For this scenario we are only interested in the data from a single partition, _start_terminal_, which we select in our filter/where clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972e773-0537-496f-9014-46e9b1227c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import os\n",
    "\n",
    "def process_partition(iterator):\n",
    "    for item in iterator:\n",
    "        sleep(0.001)\n",
    "        yield item\n",
    "\n",
    "def process_data(input_path: str) -> None:\n",
    "    df = spark.read.parquet(input_path)\n",
    "    mapped = df.rdd.mapPartitions(process_partition).toDF()\n",
    "    \n",
    "    out_df = mapped.withColumn('someCalc', F.col('Start Terminal') - F.col('End Terminal'))\n",
    "    out_df.write.mode('overwrite').parquet(output_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77ea89-4ace-432e-9d7e-5bcb8ad73e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "process_data(input_data_path_2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8753b-f8f4-4e37-ad1a-801740145d98",
   "metadata": {},
   "source": [
    "### Putting the fire out  ðŸ”¥ðŸ”¥ðŸ”¥ ðŸš’ ðŸš’ ðŸš’ ðŸ§¯ðŸ§¯ðŸ§¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c49917-7ce6-4dd2-97b6-bf790f02631e",
   "metadata": {},
   "source": [
    "So when dig into the Spark UI SQL tab, at http://localhost:4040/jobs, we see the job with a single stage and 2 tasks, it takes ~ 90 seconds to process on my laptop. Ooops, we have 6 cores available but because of our file-splits we only end up with 2 tasks (you can see this under the stage details in the UI), so only use a third of the available cores ðŸ˜­ðŸ˜­ðŸ˜­\n",
    "\n",
    "This can happen as a result of:\n",
    " * the number of input file-splits\n",
    " * repartitioning or shuffling, which could result in a small(er) number of in-memory partitions.\n",
    "\n",
    "Let's try processing the same data off of 12 file-splits ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42131e2c-fae5-4920-89e3-5cfd2785e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "process_data(input_data_path_12s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119ee91-7475-4a4c-87a9-fdc62c8c0b7d",
   "metadata": {},
   "source": [
    "### ... result  ðŸŒŸðŸŒŸðŸŒŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a4e70d-7700-4f71-848c-d71c18ef044a",
   "metadata": {},
   "source": [
    "Bonza! With this change, I see a **3x speed-up in the run-time**, in my case it is down to ~ 30 secs. \n",
    "\n",
    "Note, there are a number of Spark configuration settings which can affect the number of in-memory partitions we end up with:\n",
    "* spark.default.parallelism\n",
    "* spark.sql.shuffle.partitions\n",
    "* spark.files.maxPartitionBytes\n",
    "\n",
    "If you are not familiar with these settings it is [worth reading up and understanding how they can impact your jobs](https://spark.apache.org/docs/latest/configuration.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a030cf-0283-471d-8940-0ec81f0fe700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
