{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c5ffd1-b04c-4b5c-8287-30bab6143ad6",
   "metadata": {},
   "source": [
    "![title](img/this-is-fine-spark.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bfad4-524e-48c2-a97f-9841ace4b215",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Spark fires ðŸ”¥ - unnecessary Python UDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd267b5f-7435-43c2-8262-b76bf639330c",
   "metadata": {},
   "source": [
    "In this scenario, we will demonstrate the performance impact of using unnecessary Python UDFs when we could use SQL or Dataframe API calls instead. \n",
    "\n",
    "_What is an unnecessary UDF I hear you ask?_ An unnecessary UDF is a UDF you wrote because you were too lazy to work out how to do it in the Spark Dataframe DSL. ðŸ˜‚ - possibly a little harsh but you get the idea. There are a lot things we can do in the Spark DSLs to avoid having to write Python UDFs. \n",
    "\n",
    "Using these Python UDFs will cause data to be (de)serialized to and from the underlying Python worker executing our UDFs, which is horrible for performance. Note only that but because the UDF is a black-box, as far as Spark is c, Catalyst, views it as a black-box optimisation opportunities "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21954162-62be-4791-8e29-8a57fe38f07b",
   "metadata": {},
   "source": [
    "### Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5156a-cd85-45bb-8d79-0c614a523552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder.master(\"spark://spark:7077\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "    # .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    # .config(\"spark.eventLog.dir\", \"/data/tmp/spark-events\")\n",
    "    .appName(\"unnecessary-udfs\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768cbca-01b7-4775-b5aa-575067e1fda9",
   "metadata": {},
   "source": [
    "### Let's prep our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d9727-871d-41db-8e8a-5586ed5f3a7f",
   "metadata": {},
   "source": [
    "We are going to borrow some test data from the excellent _Spark, The Definitive Guide_ Git repo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5327d-3368-452d-923c-4307d7e771b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p /data/bike-data\n",
    "# !wget https://raw.githubusercontent.com/udacity/data-analyst/master/projects/bike_sharing/201508_station_data.csv -P /data/bike-data\n",
    "# !wget https://raw.githubusercontent.com/udacity/data-analyst/master/projects/bike_sharing/201508_trip_data.csv -P /data/bike-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2df7a28-6aa9-499b-82e6-4fd3fe377451",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data/bike-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd41dfe-f1d8-4e20-b6e5-fa909dbdfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data_path = '/data/bike-data-dates-extracted'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee650c-e678-4649-bdb5-17355dc04643",
   "metadata": {},
   "source": [
    "Next we will create some input data with two file splits for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b343ab3-3e5e-48ab-ac64-e61998b708d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array, explode, sequence, lit\n",
    "\n",
    "df = spark.read.option(\"header\", True).csv(\"/data/bike-data/201508_trip_data.csv\")\n",
    "df = df.repartition(12)\n",
    "\n",
    "# let's do a little bit of cheeky dataset expansion using the explode function\n",
    "df = df.withColumn(\"array_column\", array(sequence(lit(1), lit(2000)))) \\\n",
    "       .withColumn(\"exploded_array\", explode(\"array_column\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d4ddf-e4e8-4669-9955-b0e3f6ca5165",
   "metadata": {},
   "source": [
    "### Now let's do some data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c34eaa-4e3e-47b4-8a98-db6522132015",
   "metadata": {},
   "source": [
    "For this scenario we are only interested in the data from a single partition, _start_terminal_, which we select in our filter/where clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c972e773-0537-496f-9014-46e9b1227c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import udf, col, to_date\n",
    "\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "@udf(DateType())\n",
    "def convert_date(date_str: str) -> DateType:\n",
    "  \"\"\"Converts a date string in the format '8/19/2013' to a DateType.\"\"\"\n",
    "\n",
    "  import datetime\n",
    "\n",
    "  date_obj = datetime.datetime.strptime(date_str.split(' ')[0], '%m/%d/%Y')\n",
    "  return date_obj.date()\n",
    "\n",
    "def process_data_with_udf(df: DataFrame) -> None:\n",
    "    with_dates_df = df \\\n",
    "        .withColumn('start_date', convert_date(col('Start Date'))) \\\n",
    "        .withColumn('end_date', convert_date(col('End Date')))\n",
    "    \n",
    "    with_dates_df.write.mode('overwrite').parquet(output_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77ea89-4ace-432e-9d7e-5bcb8ad73e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "process_data_with_udf(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8753b-f8f4-4e37-ad1a-801740145d98",
   "metadata": {},
   "source": [
    "### Putting the fire out  ðŸ”¥ðŸ”¥ðŸ”¥ ðŸš’ ðŸš’ ðŸš’ ðŸ§¯ðŸ§¯ðŸ§¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c49917-7ce6-4dd2-97b6-bf790f02631e",
   "metadata": {},
   "source": [
    "I get a runtime of ~50 seconds for this, which isn't terrible but it could be better. We can avoid all the overheads of talking with the Python worker by using the PySpark Dataframe DSL instead.\n",
    "\n",
    "Let's **restart the kernel** and run again using the processing code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf73a94d-f785-44dc-9b58-0fb284eec605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_without_udf(df: DataFrame) -> None:\n",
    "    with_dates_df = df \\\n",
    "        .withColumn('start_date', to_date(col(\"Start Date\"), \"mm/dd/yyyy HH:mm\")) \\\n",
    "        .withColumn('end_date', to_date(col(\"End Date\"), \"mm/dd/yyyy HH:mm\"))\n",
    "    \n",
    "    with_dates_df.write.mode('overwrite').parquet(output_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42131e2c-fae5-4920-89e3-5cfd2785e696",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "process_data_without_udf(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119ee91-7475-4a4c-87a9-fdc62c8c0b7d",
   "metadata": {},
   "source": [
    "### ... result  ðŸŒŸðŸŒŸðŸŒŸ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a4e70d-7700-4f71-848c-d71c18ef044a",
   "metadata": {},
   "source": [
    "Whoop! With this change, my runtime is now ~30 seconds, so a **40% reduction in runtime**, nice. ðŸ‘Œ\n",
    "\n",
    "So what can we say? Well, firstly UDFs are great when you need them. But always favour the Spark DSL and Dataframe operations over UDFs where you can, as they will perform much better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3938e3-11ef-46d7-a9d1-3e3b20ec103c",
   "metadata": {},
   "source": [
    "### Apache Arrow to the rescue ðŸŽ¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a506f9-797e-4fcf-99bc-1387ca878728",
   "metadata": {},
   "source": [
    "Note, if we really have to use a UDF we can always use Apache Arrow for our UDFs as discussed in the [official Spark docs](https://spark.apache.org/docs/latest/api/python/user_guide/sql/arrow_pandas.html#arrow-python-udfs), which will in many circumstances improve performance.\n",
    "\n",
    "*Let's see what mileage we get in our case, as always let's restart the kernel and go again with the code below ...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947e188f-6ca4-4cf5-bcc0-94cea1727016",
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(DateType())\n",
    "def convert_date_arrow(date_str: str, useArrow=True) -> DateType:\n",
    "  \"\"\"Converts a date string in the format '8/19/2013' to a DateType.\"\"\"\n",
    "\n",
    "  import datetime\n",
    "\n",
    "  date_obj = datetime.datetime.strptime(date_str.split(' ')[0], '%m/%d/%Y')\n",
    "  return date_obj.date()\n",
    "\n",
    "def process_data_with_arrow_udf(df: DataFrame) -> None:\n",
    "    with_dates_df = df \\\n",
    "        .withColumn('start_date', convert_date_arrow(col('Start Date'))) \\\n",
    "        .withColumn('end_date', convert_date_arrow(col('End Date')))\n",
    "    \n",
    "    with_dates_df.write.mode('overwrite').parquet(output_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26933a2-6763-4258-91f8-5b3fce130c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "process_data_with_arrow_udf(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac181932-c5d2-41b6-ac55-f9579a6990df",
   "metadata": {},
   "source": [
    "### Wrapping up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77311d8-d90f-4e05-a923-92859d3b1e4b",
   "metadata": {},
   "source": [
    "Nice, so my runtime is down by about 5 seconds, so a **10% reduction in runtime**. ðŸ‘Œ\n",
    "\n",
    "Whilst this is a good improvement it underlines that, where possible, we are a lot better off without any Python UDFs at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a030cf-0283-471d-8940-0ec81f0fe700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c7e940-bad3-462c-a6fd-8692c3c38439",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
