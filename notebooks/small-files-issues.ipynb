{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96cb519b-95ca-496a-8001-c78d26816af8",
   "metadata": {},
   "source": [
    "![title](img/this-is-fine-spark.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43df6771-c645-459c-96f2-75da3e0d8a20",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Spark fires ðŸ”¥ - the perils of small files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f90331-5c84-4172-bd61-d7ef4f005b7e",
   "metadata": {},
   "source": [
    "In this scenario, we will demonstrate the impact of lots of small files and why you should consider adding some house-keeping schedules if your write patterns lead to this eventuality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5156a-cd85-45bb-8d79-0c614a523552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder.master(\"spark://spark:7077\")\n",
    "    # .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    # .config(\"spark.eventLog.dir\", \"/Users/owen/dev/data/tmp/spark-events\")\n",
    "    .appName(\"spark-fires-small-files\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5aaf25-58ee-44a2-a92f-91246fe04ff0",
   "metadata": {},
   "source": [
    "### Let's create some test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf5ece-57ff-4a99-88bb-91e7e50d64e2",
   "metadata": {},
   "source": [
    "So we are going to create a synthetic field which we will use to partition our data, named `pid`. We will randomely assign this across our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc1d69-db82-49ab-9a10-f653f8e918d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 720000\n",
    "num_partitions = 300\n",
    "\n",
    "df = spark.range(0, num_rows).withColumn('pid', F.floor(F.rand() * num_partitions)).cache()\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9337572-5ece-42d9-886d-61a0f1d958e3",
   "metadata": {},
   "source": [
    "Then we will save our data to two locations: one with 1 file-split per partition and at the other location we will have 12 file-splits per partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf958817-2502-4195-be50-8c38b91d5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "small_files_path = \"/data/small-files\"\n",
    "big_files_path = \"/data/big-files\"\n",
    "\n",
    "df.repartition(1).write.format(\"parquet\").mode('overwrite').partitionBy('pid').save(big_files_path)\n",
    "df.repartition(12).write.format(\"parquet\").mode('overwrite').partitionBy('pid').save(small_files_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f10490-2c2e-40aa-9f62-1b5253a7b6b9",
   "metadata": {},
   "source": [
    "### Now let's read the data and do some basic transforms on it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501bd7a5-babc-4e8a-a6fe-934e29ea3f49",
   "metadata": {},
   "source": [
    "First let's read the small files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90a5d65-d2da-4343-995c-2370de2f6bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(input_path: str) -> None:\n",
    "    mapped = spark.read.format('parquet').load(input_path)\n",
    "    mapped = mapped.withColumn('incd', F.col('id') + 1).repartition(6)\n",
    "    mapped.write.format(\"parquet\").mode('overwrite').save(\"/data/mapped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1628fc1f-631d-48c2-9f66-d1b891a4cf3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "process_data(small_files_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a75839f-e429-4fa0-a3b4-65665cf3f568",
   "metadata": {},
   "source": [
    "### Putting the fire out  ðŸ”¥ðŸ”¥ðŸ”¥ ðŸš’ ðŸš’ ðŸš’ ðŸ§¯ðŸ§¯ðŸ§¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd16a0cd-2e8c-4ff8-8203-39f0ccf8ed8a",
   "metadata": {},
   "source": [
    "Now what if we did some house-keeping and rolled up our data into fewer larger files?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a7469-d678-4843-ba32-e9b94bbfca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "process_data(big_files_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957eb554-241d-4c62-a495-f390ff453357",
   "metadata": {},
   "source": [
    "Wow, so straight away we see a **~ 4x speed increase**, boom. (For me a runtime of ~ 12 secs down to ~ 3 secs). But there are a few things to note here:\n",
    "1. So one factor is just the overhead of handling the increased number of files. \n",
    "2. Another factor, which we are not modelling here, is network latency and remote file-system/object-store interactions - again the number of files adds a significant cost. So we are missing this real-world cost in our experiment.\n",
    "3. Our processing in our noddy test is very light on computation, so the jobs are dominated by the I/O costs. So the impact of small-files will vary from application to application. That said, because all our data in this test is local we are not seeing the true cost of small-files, so real-world impacts are often very significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a030cf-0283-471d-8940-0ec81f0fe700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
