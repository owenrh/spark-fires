{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64c5ffd1-b04c-4b5c-8287-30bab6143ad6",
   "metadata": {},
   "source": [
    "![title](img/this-is-fine-spark.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29bfad4-524e-48c2-a97f-9841ace4b215",
   "metadata": {},
   "source": [
    "## ðŸ”¥ Spark fires ðŸ”¥ - caching stopping all the pushdowns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd267b5f-7435-43c2-8262-b76bf639330c",
   "metadata": {},
   "source": [
    "In this scenario, we will demonstrate how caching of dataframes can stop pushdown of partition-pruning, filters, etc, which can have pretty catastrophic impacts on the amount of data you read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21954162-62be-4791-8e29-8a57fe38f07b",
   "metadata": {},
   "source": [
    "### Bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad5156a-cd85-45bb-8d79-0c614a523552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder.master(\"spark://spark:7077\")\n",
    "    # .config(\"spark.eventLog.enabled\", \"true\")\n",
    "    # .config(\"spark.eventLog.dir\", \"/data/tmp/spark-events\")\n",
    "    .appName(\"spark-fires-premature-caching\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3768cbca-01b7-4775-b5aa-575067e1fda9",
   "metadata": {},
   "source": [
    "### Let's prep our data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75d9727-871d-41db-8e8a-5586ed5f3a7f",
   "metadata": {},
   "source": [
    "We are going to borrow some test data from the excellent _Spark, The Definitive Guide_ Git repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f5327d-3368-452d-923c-4307d7e771b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # comment this section in and execute to fetch the data\n",
    "# !mkdir -p /data/bike-data\n",
    "# !wget https://raw.githubusercontent.com/udacity/data-analyst/master/projects/bike_sharing/201508_station_data.csv -P /data/bike-data\n",
    "# !wget https://raw.githubusercontent.com/udacity/data-analyst/master/projects/bike_sharing/201508_trip_data.csv -P /data/bike-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139934c-3d85-4a49-af39-78a4ef945597",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd41dfe-f1d8-4e20-b6e5-fa909dbdfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_path = '/data/bike-data-partitioned'\n",
    "output_data_path = '/data/bike-data-partitioned-out'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ee650c-e678-4649-bdb5-17355dc04643",
   "metadata": {},
   "source": [
    "Next we will create some fake partitioning for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b343ab3-3e5e-48ab-ac64-e61998b708d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(input_data_path):\n",
    "    df = spark.read.option(\"header\", True).csv(\"/data/bike-data/201508_trip_data.csv\")\n",
    "    df = df.withColumn('start_terminal', F.col('Start Terminal'))\n",
    "    df.write.format('parquet').partitionBy('start_terminal').save(input_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7d4ddf-e4e8-4669-9955-b0e3f6ca5165",
   "metadata": {},
   "source": [
    "### Now let's do some data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c34eaa-4e3e-47b4-8a98-db6522132015",
   "metadata": {},
   "source": [
    "For this scenario we are only interested in the data from a single partition, _start_terminal_, which we select in our filter/where clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c77ea89-4ace-432e-9d7e-5bcb8ad73e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = spark.read.parquet(input_data_path)\n",
    "# df = df.cache()  # -- change 1 - remove this caching and see what happens\n",
    "\n",
    "out_df = df \\\n",
    "    .filter(F.col('start_terminal') == 83) \\\n",
    "    .select([\n",
    "        'Trip ID',\n",
    "        'Start Station',\n",
    "        'End Station',\n",
    "        'End Terminal',\n",
    "        'start_terminal'\n",
    "    ])\n",
    "out_df.cache()  # -- change 1 - add this line in\n",
    "out_df.write.mode('overwrite').parquet(output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f8753b-f8f4-4e37-ad1a-801740145d98",
   "metadata": {},
   "source": [
    "### Putting the fire out  ðŸ”¥ðŸ”¥ðŸ”¥ðŸ§¯ðŸ§¯ðŸ§¯"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c49917-7ce6-4dd2-97b6-bf790f02631e",
   "metadata": {},
   "source": [
    "So when dig into the Spark UI SQL tab, at http://localhost:4040/SQL/execution/?id=2 (your ids may differ but you want the one with a description which starts with *parquet at ...*), we see the following\n",
    " * number of files read: 420\n",
    " * scan time total (min, med, max )\n",
    " * 57.8 s (0 ms, 506 ms, 7.6 s )\n",
    " * dynamic partition pruning time: 0 ms\n",
    " * metadata time: 18 ms\n",
    " * size of files read: 10.3 MiB\n",
    " * number of output rows: 354,152\n",
    " * number of partitions read: 70\n",
    "\n",
    "Oh, how disappointing, we appear to be reading all files within all partitions despite our filter on *start_terminal*, oof! ðŸ˜ž\n",
    "\n",
    "But it's okay, we can move our caching at `-- change 1` changes, commenting one line out and the other in. Restart the kernel, recreate our Spark session and run again. Boom, now we are only reading 6 files, down from 400. In terms of bytes, we are reading << 1% of the data we were originally reading. \n",
    " \n",
    "* number of files read: 6\n",
    "* scan time total (min, med, max )\n",
    "* 8.8 s (100 ms, 1.8 s, 2.0 s )\n",
    "* dynamic partition pruning time: 0 ms\n",
    "* metadata time: 29 ms\n",
    "* size of files read: 28.0 KiB\n",
    "* number of output rows: 212\n",
    "* number of partitions read: 1\n",
    " \n",
    "In our noddy example, with all data accessed locally, **we cut the runtime by about a third**. That's not too shabby. But for real-world examples the impact can be much larger due to increased datasets and the impact of accessing data over the network from remote file-systems or object stores.\n",
    "\n",
    "Note, we get a dual impact from this anti-pattern:\n",
    "1. We read unneeded data, and so hit all the associated I/O costs.\n",
    "2. We hit all the I/O, serializations costs, etc, associated with caching data across our executors.\n",
    "\n",
    "It is worth noting the caching will impact not only partition-pruning but also column-pruning and predicate pushdown. So where possible we want to avoid caching dataframes. Where we need to cache dataframes we want to do it after our filters have been applied to avoid reading and processing unneccessary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a030cf-0283-471d-8940-0ec81f0fe700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
